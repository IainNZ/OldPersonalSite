{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual Numbers, Meet Neural Networks\n",
    "\n",
    "In this [IJulia] Jupyter Notebook, we'll be exploring how **dual numbers** can help use understand (artificial) neural networks: how they are trained, and how they work once trained. All the code in this notebook is written in the [Julia language].\n",
    "\n",
    "[IJulia]: https://github.com/JuliaLang/IJulia.jl\n",
    "[Julia language]: http://julialang.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual numbers\n",
    "\n",
    "If you are familiar with complex numbers, then dual numbers will be pretty familiar. You can think of them as an extension of the real numbers: every dual number has the form\n",
    "$$\n",
    "z = a + b \\cdot \\varepsilon,\n",
    "$$\n",
    "where I'll refer to $a$ as the real component and $b$ as the epsilon component from here on.\n",
    "The magic arises from the following property of $\\varepsilon$: $\\varepsilon^2 = 0$.\n",
    "From this property we can define all sorts of operations on dual numbers, and thats exactly what the package [DualNumbers.jl](https://github.com/JuliaDiff/DualNumbers.jl) does for the `Dual` type that it defines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using DualNumbers\n",
    "z = Dual(2,3)\n",
    "@show z\n",
    "@show real(z)\n",
    "@show epsilon(z)\n",
    "y = Dual(4,5)\n",
    "@show y\n",
    "@show z + y\n",
    "@show z * y;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last example, we can see that the $3\\varepsilon \\times 5\\varepsilon$ term disappeared, as expected.\n",
    "\n",
    "Dual numbers have various uses, but the one we are interested here is that it enables [automatic diferentiation](http://www.autodiff.org/) of functions. We'll skip the math ([see Wikipedia for a decent if brief explanation](https://en.wikipedia.org/wiki/Dual_number#Differentiation)), but loosely: if we pass a dual number $x + \\varepsilon$ instead of a real number $x$ into a function $f$, we not only get the value of the function $f(x)$ but also *the first derivative at $x$*. In particular, the result will also be a dual number, and the epsilon component of that dual number is the derivative. Consider the following simple examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "square(5.0) = 25.0\n",
      "square(Dual(5.0,1.0)) = 25.0 + 10.0du\n",
      "2.0 * 5.0 = 10.0\n",
      "sin(0.75π) = 0.7071067811865476\n",
      "sin(Dual(0.75π,1)) = 0.7071067811865476 - 0.7071067811865475du\n",
      "cos(0.75π) = -0.7071067811865475\n"
     ]
    }
   ],
   "source": [
    "square(x) = x^2\n",
    "@show square(5.0)\n",
    "@show square(Dual(5.0,1.0))\n",
    "@show 2.0*5.0\n",
    "\n",
    "@show sin(0.75π)\n",
    "@show sin(Dual(0.75π,1))\n",
    "@show cos(0.75π);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is happening here is that we are defining a Julia function `square` that takes any input, and tries to square it. When we pass in a `Float64` (`5.0`), Julia just-in-time compiles a version of the function for `Float64`s. The neat thing is that it will do the same thing for `Dual`s too! We can even look at the code that is generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t.section\t__TEXT,__text,regular,pure_instructions\n",
      "Filename: In[2]\n",
      "Source line: 1\n",
      "\tpushq\t%rbp\n",
      "\tmovq\t%rsp, %rbp\n",
      "Source line: 1\n",
      "\tvmulsd\t%xmm0, %xmm0, %xmm0\n",
      "\tpopq\t%rbp\n",
      "\tret\n"
     ]
    }
   ],
   "source": [
    "@code_native square(5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mulsd` is essentially saying take the input, and multiply it with itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t.section\t__TEXT,__text,regular,pure_instructions\n",
      "Filename: In[2]\n",
      "Source line: 1\n",
      "\tpushq\t%rbp\n",
      "\tmovq\t%rsp, %rbp\n",
      "Source line: 1\n",
      "\tvmovsd\t(%rdi), %xmm0\n",
      "\tvmovsd\t8(%rdi), %xmm1\n",
      "Source line: 1\n",
      "\tvaddsd\t%xmm1, %xmm1, %xmm1\n",
      "\tvmulsd\t%xmm1, %xmm0, %xmm1\n",
      "\tvmulsd\t%xmm0, %xmm0, %xmm0\n",
      "\tpopq\t%rbp\n",
      "\tret\n"
     ]
    }
   ],
   "source": [
    "@code_native square(Dual(5.0,1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we add the second input to itself (`1+1=2`), then multiply it by the first input (`2*5=10`, the derivative), then multiply the first input by itself (`5*5=25`, the value). So essentially, the minimum possible code. This means using dual numbers in Julia can a very efficient way of getting derivatives without any extra work on the users part. To demonstrate this, consider the following more complicated example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squareroot(9.0) = 3.0\n",
      "squareroot(Dual(9.0,1.0)) = 3.0 + 0.16666666666666669du\n",
      "0.5 * 9 ^ -0.5 = 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "function squareroot(a)\n",
    "    x = a\n",
    "    while abs(x*x - a) > 1e-10\n",
    "        x = (x + a/x)/2\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "@show squareroot(9.0)\n",
    "@show squareroot(Dual(9.0,1.0))\n",
    "# f (x) = x^0.5\n",
    "# f'(x) = 0.5*x^-0.5\n",
    "@show 0.5*9^-0.5 ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Networks with Dual Numbers\n",
    "\n",
    "Today we are going to going to consider the problem of *binary classification*. Suppose we have a set of cases $(x_1,y_1),(x_2,y_2),\\dots$ where each $x$ is a vector of length $n$ and $y$ is either 0 or 1. We'd like a function $f(x)$ that tells us whether $x$ is a 0 or a 1, and we'd like to learn a function that does this from the data we have. An example application would the important problem of **cat detection**: given an image, which we describe as a series of numbers describing the color of each pixel, is a cat present in the image or not?\n",
    "\n",
    "Artificial neural networks (ANNs) are, for the purposes of this notebook and very briefly, a particular family of these functions. If you aren't familiar with them, check out the [Wikipedia page](https://en.wikipedia.org/wiki/Artificial_neural_network) or this [EBook](http://neuralnetworksanddeeplearning.com/index.html). We have a *layers* of *neurons*, where a neuron is a simple, and usually nonlinear, function of its weighted inputs. Thus, if we fix the structure of our ANN, we are then optimizing over a family of possible functions, where these functions are defined by these weights. Thus, given weights $w$ and an input $x$, my neural network is the function $g(w,x)$ that gives me a number. We're doing classification, so this number will between 0 and 1 and we'll round it to determine it is class 0 or class 1.\n",
    "\n",
    "To explore neural networks, we'll first create some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll consider a dimension-2 input space:\n",
    "#        (1,1)\n",
    "#    +----+\n",
    "#    |\\ 1 |\n",
    "#    | \\  |\n",
    "#    |  \\ |\n",
    "#    | 0 \\|\n",
    "#    +----+\n",
    "# (0,0)\n",
    "# Where class 0s are below the diagonal, and 1s are above\n",
    "m = 50  # Number of training samples\n",
    "srand(1988)  # So we always get same data\n",
    "X = rand(m,2)  # m rows, n columns, in [0,1]\n",
    "# Generate class labels\n",
    "y = [(X[i,1] + X[i,2] <= 1 ? 0.0 : 1.0) for i in 1:m];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now layout our neural network. It'll be very simple:\n",
    "\n",
    "* 2 inputs.\n",
    "* 4 hidden neurons. Each input connects to each hidden neuron, and each neuron is a sigmoidal function of its weighted inputs.\n",
    "* 1 output neuron, also a sigmoidal function of its weighted inputs.\n",
    "\n",
    "We thus need to find values for 12 weights.\n",
    "Lets make a function that calculates a classification, given weights and an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ann (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x) = 1/(1 + exp(-x))\n",
    "# w: 12x1 vector of weights\n",
    "# x:  2x1 vector of inputs\n",
    "function ann(w,x)\n",
    "    # Calculate value at hidden neurons\n",
    "    h1 = sigmoid(dot(w[1:2],x))\n",
    "    h2 = sigmoid(dot(w[3:4],x))\n",
    "    h3 = sigmoid(dot(w[5:6],x))\n",
    "    h4 = sigmoid(dot(w[7:8],x))\n",
    "    # Calculate and return value at output\n",
    "    return sigmoid(dot(w[9:12],[h1,h2,h3,h4]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann(w,[0.0,0.0]) = 0.3044973857244263\n",
      "round(ann(w,[0.0,0.0])) = 0.0\n",
      "ann(w,[1.0,1.0]) = 0.28431412765544134\n",
      "round(ann(w,[1.0,1.0])) = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Test out our new ANN with some random weights\n",
    "srand(1988)\n",
    "w = rand(12) - 0.5  # Between [-0.5, +0.5]\n",
    "@show ann(w,[0.0,0.0])\n",
    "@show round(ann(w,[0.0,0.0]))  # 0 -> Correct :D\n",
    "@show ann(w,[1.0,1.0])\n",
    "@show round(ann(w,[1.0,1.0]))  # 0 -> Not correct D:\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we find these weights? We must first formalize what it is we are trying to achieve. In this example, we'll consider *square loss*. In particular, given an input $x$, a true class $y$, and the neural networks guess $z$, the error for this input is $(y-z)^2$. The total error across all inputs is the sum of this quantity across all our cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "annerror (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w: 12x1 vector of weights\n",
    "# X:  mx2 matrix of inputs\n",
    "# y:  mx1 vector of true labels\n",
    "function annerror(w,X,y)\n",
    "    err = 0.0\n",
    "    for i in 1:length(y)\n",
    "        z = ann(w,vec(X[i,:]))\n",
    "        err += (y[i] - z)^2\n",
    "    end\n",
    "    return err\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annerror(w,X,y) = 13.983915288773288\n",
      "annerror(w,X,y) = 12.488382535614333\n",
      "annerror(w,X,y) = 13.898796784329502\n"
     ]
    }
   ],
   "source": [
    "# Test out error with some random weights\n",
    "srand(1988)\n",
    "w = rand(12) - 0.5\n",
    "@show annerror(w,X,y)\n",
    "w = rand(12) - 0.5\n",
    "@show annerror(w,X,y)\n",
    "w = rand(12) - 0.5\n",
    "@show annerror(w,X,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a notion of error, we now have something we can minimize. \"Real\" neural networks use *backpropogation* to do this, often coupled with *stochastic gradient descent*. To learn about these things, I'd suggest [this ebook](http://neuralnetworksanddeeplearning.com/index.html). Here we're just going to treat the error of our ANN as a plain old nonlinear function, and minimize that error with respect to the cases and weights using *gradient descent*. In gradient descent, we start at some $x$ and calculate the derivative at $x$, $f^\\prime(x)$. We then update $x$ by setting it equal to $x-\\lambda f^\\prime(x)$, where $\\lambda$ is a \"small\" number. Intutively, the derivate points in the \"direction\" that the function is increasing in, so by moving in the opposite direction we should decrease the value of our function. Fortunately, we have a pretty handy way of calculating derivatives: **dual numbers**!\n",
    "\n",
    "What we need to do is to evaluate the neural network 12 times with dual number weights. In each of the 12 calculations most of the weights will just be plain numbers, except one which will have a epsilon component - thats the one we are calculating the derivative with respect to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "annderiv (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w: 12x1 vector of weights\n",
    "# X:  mx2 matrix of inputs\n",
    "# y:  mx1 vector of true labels\n",
    "function annderiv(w,X,y)\n",
    "    err = 0.0\n",
    "    deriv = zeros(12)\n",
    "    for j in 1:12\n",
    "        # Create a new w that is dual numbers\n",
    "        # but with 0 epsilon component...\n",
    "        w_dual = [Dual(w_k) for w_k in w]\n",
    "        # Except for the jth component\n",
    "        w_dual[j] = Dual(w[j],1)\n",
    "        # Calculate the error for this w,\n",
    "        # with respect to w_j\n",
    "        err_j = annerror(w_dual,X,y)\n",
    "        # Extract the derivative and error\n",
    "        deriv[j] = epsilon(err_j)\n",
    "        err = real(err_j)  # This doesn't change with j\n",
    "    end\n",
    "    return err, deriv\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annerror(w,X,y) = 13.983915288773288"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12-element Array{Float64,1}:\n",
       "  0.275391\n",
       "  0.27482 \n",
       "  0.344183\n",
       "  0.343772\n",
       "  0.397583\n",
       "  0.395535\n",
       "  0.386477\n",
       "  0.38205 \n",
       " -0.864485\n",
       " -2.41978 \n",
       " -1.94487 \n",
       " -2.1693  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "err = 13.983915288773288\n"
     ]
    }
   ],
   "source": [
    "srand(1988)\n",
    "w = rand(12) - 0.5\n",
    "@show annerror(w,X,y)\n",
    "err, deriv = annderiv(w,X,y)\n",
    "@show err  # matches\n",
    "deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate derivatives, so all thats left is to \"train\" our neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "anntrain (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w: 12x1 vector of weights (initial guess)\n",
    "# X:  mx2 matrix of inputs\n",
    "# y:  mx1 vector of true labels\n",
    "# λ: learning rate\n",
    "# τ: tolerance for stopping\n",
    "function anntrain(w,X,y;λ=0.2,τ=1e-3)\n",
    "    err, deriv = annderiv(w,X,y)\n",
    "    println(\"Initial error: \", err)\n",
    "    while true\n",
    "        # Take step\n",
    "        w -= λ*deriv\n",
    "        # Caclulate new error, derivative\n",
    "        new_err, new_deriv = annderiv(w,X,y)\n",
    "        if new_err > err\n",
    "            # We got worse!\n",
    "            println(\"Stopping, error got worse! \", new_err)\n",
    "            break\n",
    "        elseif abs(new_err - err) <= τ\n",
    "            # About the same\n",
    "            println(\"Stopping, error didn't change enough! \", new_err)\n",
    "            break\n",
    "        end\n",
    "        err, deriv = new_err, new_deriv\n",
    "    end\n",
    "    println(\"Final error: \", err)\n",
    "    return err, w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error: 13.983915288773288\n",
      "Stopping, error got worse! 3.394513196126495\n",
      "Final error: 3.3781551765954303\n",
      "opt_err = 3.3781551765954303\n",
      "opt_w = [-1.3558493302790593,-1.2135224028308582,1.3790390467470208,1.2433303868712935,-1.3010050172611491,-1.2881918785307962,0.705890863547658,1.159733839452436,-8.932529517332856,3.151012786161526,-6.278927931383467,0.8465181644415757]\n"
     ]
    }
   ],
   "source": [
    "srand(1988)\n",
    "init_w = rand(12) - 0.5\n",
    "opt_err, opt_w = anntrain(init_w,X,y)\n",
    "@show opt_err\n",
    "@show opt_w;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see how our network does in terms of accuracy\n",
    "ann_y = Float64[ann(opt_w,vec(X[i,:])) for i in 1:m]\n",
    "ann_y = round(ann_y)\n",
    "correct = sum(ann_y .== y)\n",
    "accuracy = correct / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network correctly classifies 98% of the input samples. However, the real test is how well it performs on data it has never seen before (*out-of-sample testing*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srand(1234)  # So we always get same data, but not the same as train\n",
    "X_test = rand(m,2)  # m rows, n columns, in [0,1]\n",
    "y_test = [(X_test[i,1] + X_test[i,2] <= 1 ? 0.0 : 1.0) for i in 1:m]\n",
    "ann_y_test = Float64[ann(opt_w,vec(X_test[i,:])) for i in 1:m]\n",
    "ann_y_test = round(ann_y_test)\n",
    "correct_test = sum(ann_y_test .== y_test)\n",
    "accuracy_test = correct_test / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 96% accuracy on fresh samples, which is pretty good - it means our neural network has probably *generalized* fairly well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Neural Networks with Dual Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have so far being using dual numbers on the weights, but there is no reason we can't use dual numbers on the inputs as well. For example, consider calculating the derivative with respect to the inputs in the center of the input space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4509053507079046 + 1.0604056636074148du"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann(opt_w,[Dual(0.5,1), Dual(0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4509053507079046 + 1.0051736585860678du"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann(opt_w,[Dual(0.5), Dual(0.5,1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense: when we are in the center, increasing either component of the input will make us more like class 1.\n",
    "You may have seen [Google's \"inceptionism\"](http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html) - we can use dual numbers in the same way here. Consider a \"random point\" that we want to make more like class 1 - what direction should we go in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "anninputderiv (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anninputderiv(w,x) = [\n",
    "    epsilon(ann(w,[Dual(x[1],1),Dual(x[2])])),\n",
    "    epsilon(ann(w,[Dual(x[1]),Dual(x[2],1)]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann(opt_w,x1) = 0.06551264206809532\n",
      "d1 = [0.3492604450563003,0.3301696511678158]\n"
     ]
    }
   ],
   "source": [
    "x1 = [0.2,0.3]\n",
    "@show ann(opt_w,x1)\n",
    "d1 = anninputderiv(opt_w,x1)\n",
    "@show d1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2 = [0.37463022252815015,0.4650848255839079]\n",
      "ann(opt_w,x2) = 0.28584906572626806\n",
      "d2 = [0.976891112109046,0.9248478101055944]\n"
     ]
    }
   ],
   "source": [
    "x2 = x1 + 0.5d1\n",
    "@show x2\n",
    "@show ann(opt_w,x2)\n",
    "d2 = anninputderiv(opt_w,x2)\n",
    "@show d2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x3 = [0.8630757785826731,0.9275087306367051]\n",
      "ann(opt_w,x3) = 0.901051341059385\n",
      "d3 = [0.1871854954638273,0.17865325553691935]\n"
     ]
    }
   ],
   "source": [
    "x3 = x2 + 0.5d2\n",
    "@show x3\n",
    "@show ann(opt_w,x3)\n",
    "d3 = anninputderiv(opt_w,x3)\n",
    "@show d3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x4 = [0.9566685263145868,1.0168353584051648]\n",
      "ann(opt_w,x4) = 0.9276051137579988\n",
      "d4 = [0.11598601972701265,0.11093992991268177]\n"
     ]
    }
   ],
   "source": [
    "x4 = x3 + 0.5d3\n",
    "@show x4\n",
    "@show ann(opt_w,x4)\n",
    "d4 = anninputderiv(opt_w,x4)\n",
    "@show d4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've completely changed the class by following the dual number derivatives - from a definite 0, to a definite 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0-dev",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
